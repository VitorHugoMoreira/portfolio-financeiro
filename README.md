# portfolio-financeiro

Perfeito ğŸ™Œ
Aqui estÃ¡ o **README.md pronto em Markdown**, jÃ¡ formatado para vocÃª usar no GitHub:

---

```markdown
# ğŸ“Š Pipeline de Dados no Azure â€“ ConcessÃµes de CrÃ©dito

## ğŸš€ VisÃ£o Geral
Este repositÃ³rio apresenta um **pipeline completo de dados bancÃ¡rios** implementado no **Microsoft Azure**, combinando **Azure Data Factory (ADF)**, **Azure Databricks** e **Azure DevOps**.  
O objetivo foi construir uma soluÃ§Ã£o ponta a ponta para **ingestÃ£o, processamento, versionamento e anÃ¡lise** de dados pÃºblicos do **Banco Central do Brasil** sobre **concessÃµes de crÃ©dito**.

A arquitetura foi organizada em trÃªs camadas principais:  
1. **IngestÃ£o (ADF)** â†’ Dados SQL on-premises e em nuvem ingeridos no **Azure Data Lake** (camada raw/bronze).  
2. **Processamento (Databricks + PySpark)** â†’ Limpeza, transformaÃ§Ã£o e armazenamento confiÃ¡vel em **Delta Lake**.  
3. **GovernanÃ§a e Versionamento (DevOps + GitHub)** â†’ Notebooks versionados, pipelines integrados ao **DevOps** e prÃ¡ticas de **CI/CD**.  

---

## ğŸ› ï¸ Tecnologias Utilizadas
- **Azure Data Factory (ADF)** â€“ OrquestraÃ§Ã£o e ingestÃ£o de dados.  
- **Azure Databricks** â€“ Processamento distribuÃ­do com PySpark e SQL.  
- **Delta Lake** â€“ Armazenamento confiÃ¡vel e escalÃ¡vel.  
- **Azure Data Lake Storage Gen2** â€“ Data lake estruturado em camadas (raw/bronze).  
- **Integration Runtime** â€“ ConexÃ£o segura com ambientes on-premises.  
- **Azure DevOps + GitHub** â€“ Versionamento de notebooks e pipelines, CI/CD.  
- **PySpark / Spark SQL** â€“ Limpeza, transformaÃ§Ã£o e anÃ¡lise.  

---

## ğŸ“‚ Estrutura do RepositÃ³rio
```

azure-credit-pipeline/
â”‚
â”œâ”€â”€ README.md                  # DocumentaÃ§Ã£o principal
â”‚
â”œâ”€â”€ notebooks/                 # Notebooks PySpark e SQL
â”‚   â”œâ”€â”€ 01\_ingestao\_raw\.py
â”‚   â”œâ”€â”€ 02\_transformacao\_bronze.py
â”‚   â””â”€â”€ 03\_analise\_credito.sql
â”‚
â”œâ”€â”€ pipelines\_adf/             # Pipelines do Data Factory (simulados em JSON)
â”‚   â”œâ”€â”€ linked\_services.json
â”‚   â”œâ”€â”€ datasets.json
â”‚   â””â”€â”€ pipeline\_movimentacao.json
â”‚
â”œâ”€â”€ devops/                    # IntegraÃ§Ã£o e automaÃ§Ã£o (CI/CD)
â”‚   â”œâ”€â”€ ci\_pipeline.yaml
â”‚   â”œâ”€â”€ cd\_pipeline.yaml
â”‚   â””â”€â”€ git\_integration\_steps.md
â”‚
â””â”€â”€ data/                      # Dados de exemplo
â”œâ”€â”€ raw/                   # CSV do Banco Central (entrada)
â””â”€â”€ bronze/                # Dados tratados em Delta/Parquet

````

---

## ğŸ”„ Pipeline do Projeto

### 1. IngestÃ£o â€“ **Azure Data Factory**
- ConfiguraÃ§Ã£o de **linked services** (SQL Server on-premises + Blob Storage).  
- CriaÃ§Ã£o de **datasets** (SQL + arquivos CSV).  
- Desenvolvimento de **pipelines** para mover dados para o Data Lake.  

### 2. Processamento â€“ **Azure Databricks**
Exemplo de transformaÃ§Ã£o em PySpark:
```python
from pyspark.sql.functions import col, to_date, date_format

# Leitura
df = spark.read.csv("/mnt/datalake/raw/concessoes_credito.csv", header=True, inferSchema=True)

# TransformaÃ§Ã£o
df_clean = df.withColumn("Data", to_date(col("Data"), "yyyy-MM-dd")) \
             .withColumn("AnoMes", date_format(col("Data"), "yyyy-MM"))

# Escrita no Delta
df_clean.write.format("delta").mode("overwrite").save("/mnt/datalake/bronze/concessoes_credito_delta")
````

### 3. AnÃ¡lise â€“ **SQL no Databricks**

```sql
SELECT AnoMes, SUM(Valor) AS TotalConcedido
FROM concessoes_credito
GROUP BY AnoMes
ORDER BY AnoMes;
```

### 4. Versionamento â€“ **Azure DevOps + GitHub**

* RepositÃ³rio Git integrado ao ADF.
* Pipelines CI/CD para testes e deploy de notebooks.
* Versionamento automÃ¡tico de JSONs e backups de pipelines.

---

## ğŸ“Š Insights Obtidos

* **Sazonalidade**: padrÃµes mensais nas concessÃµes de crÃ©dito.
* **Impacto econÃ´mico**: variaÃ§Ãµes de acordo com contexto macroeconÃ´mico.
* **EficiÃªncia**: avaliaÃ§Ã£o da performance no processamento distribuÃ­do.

---

## ğŸš€ Possibilidades Futuras

* IntegraÃ§Ã£o com **Power BI** para dashboards avanÃ§ados.
* AutomaÃ§Ã£o de execuÃ§Ãµes com **Databricks Jobs**.
* IntegraÃ§Ã£o com **MLflow** para modelagem de risco de crÃ©dito.

---

## ğŸ”— Fontes de Dados

* [Banco Central do Brasil â€“ ConcessÃµes de CrÃ©dito (SCR)](https://dadosabertos.bcb.gov.br/dataset/20631-concessoes-de-credito---total)

